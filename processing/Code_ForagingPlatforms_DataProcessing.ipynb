{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Script for: Continuous foraging behavior shapes patch-leaving decisions in pigeons: A 3D tracking study\n",
    "\n",
    "Guillermo Hidalgo Gadea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal foraging behavior is a key component of successful adaptations to natural environments. Understanding how animals decide to stay near food or to leave it for another food patch gives us insights into the underlying cognitive mechanisms that govern adaptive behaviors. 3D pose tracking was used to determine how pigeons exploit a 4 square meter arena with two separate platforms (i.e. food patches) whose absolute and relative elevations were manipulated. Detailed kinematic features of foraging and traveling behaviors were quantified using automated video tracking, without a need for manual coding. Our computational approach captured continuous, high-dimensional movement patterns and enabled precise quantification of travel costs between patches. Combined with mixed-effects survival analysis, our detailed behavioral tracking provided unprecedented insight into the moment-by-moment dynamics of patch-leaving decisions of pigeons. As expected from behavior optimization models, our results showed a preference to visit a ground food platform first, and longer latencies to leave an elevated platform. Foraging activity significantly decreased throughout a session, with shorter visits, less pecks per visit, and a decrease in inter-peck variability. However, a mixed-effects Cox regression modeled pigeons' patch-leaving probability, demonstrating that current and cumulative foraging parameters between patches significantly enhanced the model's predictive power beyond patch accessibility (i.e., beyond travel costs). This suggests that pigeons integrate both current environmental cues and their individual foraging history when making patch-leaving decisions. Our findings are discussed in relation to the marginal value theorem and optimal foraging theory.\n",
    "\n",
    "`updated 22.08.2025`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Demo Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries ##\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "np.random.seed(1234)\n",
    "\n",
    "# load MotionPype\n",
    "from motionpype import behaviorspecific, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Project Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_project_structure(projectpath):\n",
    "    # find metadata\n",
    "    metadatafile = utils.scrapdirbystring(os.path.dirname(projectpath), 'metadata.csv', output = False)[0]\n",
    "    metadata = pd.read_csv(metadatafile, sep=',')\n",
    "    metadata['date'] = [d.split('T')[0].split('-')[0]+d.split('T')[0].split('-')[1]+d.split('T')[0].split('-')[2] for d in metadata['date'].values]\n",
    "    # find pose files\n",
    "    pose_files = [file for file in utils.scrapdirbystring(projectpath, 'csv', output = False) if 'pose-3d' in file]\n",
    "    sessions = []\n",
    "    PIDs = []\n",
    "    for file in pose_files:\n",
    "        # get relative path to projectpath\n",
    "        relpath = os.path.relpath(file, projectpath)\n",
    "        dir = os.path.dirname(os.path.dirname(relpath))\n",
    "        PIDs.append(os.path.basename(dir))\n",
    "        sessions.append(os.path.dirname(dir))\n",
    "    # print info\n",
    "    print(f'Found {len(pose_files)} files in {projectpath}')\n",
    "    print(f'Metadata found in {metadatafile}')\n",
    "    print(f'with {len(set(sessions))} sessions: {set(sessions)}')\n",
    "    print(f'from {len(set(PIDs))} Pigeons: {set(PIDs)}')\n",
    "\n",
    "    return metadata, pose_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set project parameters\n",
    "projectpath = r\"K:\\ForagingPlatformsArena_local\\Triangulation\"\n",
    "outputpath = r\"K:\\ForagingPlatformsArena_local\\Analysis\"\n",
    "\n",
    "# read project structure\n",
    "metadata, pose_files = read_project_structure(projectpath)\n",
    "\n",
    "# set recording parameters (do not include paranthesis in bodypart string)\n",
    "reference_points = ['cA','cB','cC','cD','cE','cF']\n",
    "head = ['Head', 'UpperCere', 'LowerCere', 'BeakTip', 'LeftEye', 'RightEye', 'UpperNeck']\n",
    "body = ['UpperSpine=LN', 'LowerSpine', 'MiddleSpine', 'UpperHalfSpine',\n",
    "        'LowerHalfSpine', 'TailLeft', 'TailRight', 'TailCenter']\n",
    "\n",
    "# Smoothing parameters\n",
    "fps = 50 # in Hz\n",
    "\n",
    "# Demo dataset\n",
    "file = pose_files[10]\t\n",
    "# DEBUGGING\n",
    "#file =  r'K:\\ForagingPlatformsArena_local\\Triangulation\\20221101\\P195\\pose-3d\\h265_crf12_20221101_ForagingPlatforms_P195.csv'\n",
    "#file = r'K:\\ForagingPlatformsArena_local\\Triangulation\\20230220\\P122\\pose-3d\\h265_crf12_20230220_ForagingPlatforms_P122.csv'\n",
    "PID = os.path.splitext(os.path.basename(file))[0].split('_')[-1]\n",
    "date = os.path.splitext(os.path.basename(file))[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Session Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get PID from file\n",
    "def read_metadata(metadata, PID, date):\n",
    "    # subset metadata\n",
    "    idx1 = (metadata['pigeon_id']==PID).values\n",
    "    idx2 = (metadata['date']==date).values\n",
    "\n",
    "    entry = metadata[idx1&idx2]\n",
    "    if len(entry) < 1:\n",
    "        print('error, case not found!')\n",
    "    elif len(entry) > 1:\n",
    "        print('error, index not unique!')\n",
    "    else:\n",
    "        #extract metadata\n",
    "        condition = entry['session_type.f'].values[0]\n",
    "        session = entry['session_number'].values\n",
    "        food_consumed = int(entry['food_depleted'] - entry['food_other'])\n",
    "        depleted_A = int(entry['food_depleted_pos2'])\n",
    "        depleted_B = int(entry['food_depleted_pos1'])\n",
    "        depleted_high = food_consumed *0\n",
    "        depleted_low = food_consumed *0\n",
    "        if int(condition.split('-')[1]) >= 0:\n",
    "            depleted_high += depleted_A\n",
    "        if int(condition.split('-')[0]) >= 0:\n",
    "            depleted_high += depleted_B\n",
    "        if int(condition.split('-')[1]) <= 1: \n",
    "            depleted_low += depleted_A\n",
    "        if int(condition.split('-')[0]) <= 1: \n",
    "            depleted_low += depleted_B\n",
    "        \n",
    "        sex = entry['sex'].values\n",
    "        age = entry['age'].values\n",
    "        colony = entry['colony'].values\n",
    "        cohort = entry['cohort'].values\n",
    "        normal_weight = entry['normal_weight'].values\n",
    "        weight = entry['weight_g'].values\n",
    "\n",
    "    # platform A on wall BC (left from door) and platform B on wall EF (right from door)\n",
    "    elevation_A = int(condition.split('-')[1])*10\n",
    "    elevation_B = int(condition.split('-')[0])*10\n",
    "\n",
    "    return condition, session, food_consumed, depleted_A, depleted_B, depleted_high, depleted_low, sex, age, colony, cohort, normal_weight, weight, elevation_A, elevation_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition, session, food_consumed, depleted_A, depleted_B, depleted_high, depleted_low, sex, age, colony, cohort, normal_weight, weight, elevation_A, elevation_B = read_metadata(metadata, PID, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Pigeon over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_pigeon(file, fps, reference_points, head, body):\n",
    "    # read 3D data from csv file\n",
    "    error, ncams, score, reference, pose = behaviorspecific.read_anipose_data(file, reference_points)\n",
    "\n",
    "    # calculate stable centroids from filtered data\n",
    "    behavior_smoothing = 300 # in ms, shortest meaningful behavior interval\n",
    "    body_smooting =  int(np.ceil(behavior_smoothing/1000 *fps) // 2 * 2 + 1)\n",
    "    filtered_head = behaviorspecific.median_offset_filtering(pose, head,  k_tracking = 41, max_dist = 60, interpolation = 'linear')\n",
    "    centroid_head = behaviorspecific.reduceposetocentroid(filtered_head, head, med_smooth = 3)\n",
    "    filtered_body = behaviorspecific.median_offset_filtering(pose, body,  k_tracking = 41, max_dist = 60, interpolation = 'linear')\n",
    "    centroid_body = behaviorspecific.reduceposetocentroid(filtered_body, body, med_smooth = body_smooting)\n",
    "\n",
    "    # calculate displacement as euclidean distance between frame-to-frame positions\n",
    "    centroid_body_displacement = np.diff(centroid_body, n = 1, axis = 0, prepend = np.array([centroid_body.iloc[0,:]]))\n",
    "    euclidean_body_displacement = np.sqrt(np.sum(centroid_body_displacement**2, axis = 1)) \n",
    "    centroid_head_displacement = np.diff(centroid_head, n = 1, axis = 0, prepend = np.array([centroid_head.iloc[0,:]]))\n",
    "    euclidean_head_displacement = np.sqrt(np.sum(centroid_head_displacement**2, axis = 1))\n",
    "\n",
    "    # save tracking to data frame\n",
    "    centroid_head.columns=['centroid_head_x', 'centroid_head_y', 'centroid_head_z']\n",
    "    centroid_body.columns=['centroid_body_x', 'centroid_body_y', 'centroid_body_z']\n",
    "    df_tracking = pd.concat([centroid_head, centroid_body], axis=1)\n",
    "    # append body displacement\n",
    "    df_tracking['euclidean_head_disp'] = euclidean_head_displacement\n",
    "    df_tracking['euclidean_body_disp'] = euclidean_body_displacement\n",
    "\n",
    "    return df_tracking, pose, reference, filtered_head, filtered_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add visualization \n",
    "df_tracking, pose, reference, filtered_head, filtered_body = track_pigeon(file, fps, reference_points, head, body)\n",
    "df_tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Platform Visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_visits(df_tracking, reference, fps, elevation_A, elevation_B):\n",
    "    # Platform ROI dimensions TODO\n",
    "    width = 600 # in mm\n",
    "    depth = 500\n",
    "    height = 500 # padding comes on top\n",
    "    padding = 50 # error around borders\n",
    "    smooth_states = 2000 # in ms, shortest meaningful duration for a visit\n",
    "    k_states =  int(np.ceil(smooth_states/1000 *fps) // 2 * 2 + 1)\n",
    "\n",
    "    # extract centroid_head\n",
    "    centroid_head = df_tracking[['centroid_head_x', 'centroid_head_y', 'centroid_head_z']]\n",
    "\n",
    "    # track pigeon in platform ROI\n",
    "    p1 = np.array(reference.loc[0,reference.columns.str.contains(\"cB\")])\n",
    "    p2 = np.array(reference.loc[0,reference.columns.str.contains(\"cC\")])\n",
    "    platform_A = behaviorspecific.findplatformROI(p1, p2, width, height, depth, padding, elevation_A)\n",
    "    on_platform_A = behaviorspecific.findpointin3DROI(centroid_head, platform_A, smooth = k_states)\n",
    "\n",
    "    q1 = np.array(reference.loc[0,reference.columns.str.contains(\"cE\")])\n",
    "    q2 = np.array(reference.loc[0,reference.columns.str.contains(\"cF\")])\n",
    "    platform_B = behaviorspecific.findplatformROI(q1, q2, width, height, depth, padding, elevation_B)\n",
    "    on_platform_B = behaviorspecific.findpointin3DROI(centroid_head, platform_B, smooth =k_states)\n",
    "\n",
    "    # save tracking to data frame\n",
    "    df_tracking['on_platform'] = [a or b for a, b in zip(on_platform_A, on_platform_B)]\n",
    "    df_tracking['on_platform_A'] = on_platform_A\n",
    "    df_tracking['on_platform_B'] = on_platform_B\n",
    "\n",
    "    # calculate transitions\n",
    "    df_tracking['transition_in'] = 0\n",
    "    df_tracking['transition_out']= 0\n",
    "    df_tracking.loc[np.diff(df_tracking['on_platform'], prepend = 0)>0, 'transition_in'] = 1\n",
    "    df_tracking.loc[np.diff(df_tracking['on_platform'], prepend = 0)<0, 'transition_out'] = 1\n",
    "     \n",
    "    # subset visits\n",
    "    visits = df_tracking[df_tracking['transition_in']==1]\n",
    "    frames = list(visits.index)\n",
    "\n",
    "    # calculate self transitions\n",
    "    df_tracking['transition_self'] = 0\n",
    "    idx1 = np.diff(visits['on_platform_A'], prepend=True)*1\n",
    "    idx2 = np.diff(visits['on_platform_B'], prepend=True)*1\n",
    "    selftransitions = abs(idx2*idx1 -1)\n",
    "    if len(selftransitions) > 0:\n",
    "        selftransitions[0] = 0\n",
    "    df_tracking.loc[frames, 'transition_self'] = selftransitions\n",
    "\n",
    "    # assign platform identity\n",
    "    df_tracking.loc[df_tracking['on_platform']==False, 'location'] = 'arena'\n",
    "    df_tracking.loc[df_tracking['on_platform_A']==True, 'location'] = 'platform A'\n",
    "    df_tracking.loc[df_tracking['on_platform_B']==True, 'location'] = 'platform B'\n",
    "\n",
    "    # assign platform height\n",
    "    df_tracking.loc[df_tracking['on_platform']==False, 'elevation'] = 0\n",
    "    df_tracking.loc[df_tracking['on_platform_A']==True, 'elevation'] = elevation_A\n",
    "    df_tracking.loc[df_tracking['on_platform_B']==True, 'elevation'] = elevation_B\n",
    "\n",
    "    # calculate visit order\n",
    "    df_tracking['visit_order'] = 0\n",
    "    df_tracking['visit_order_A'] = 0\n",
    "    df_tracking['visit_order_B'] = 0\n",
    "    visit_order = np.cumsum(visits['on_platform'])\n",
    "    visit_order_A = np.cumsum(visits['on_platform_A'])\n",
    "    visit_order_B = np.cumsum(visits['on_platform_B'])\n",
    "    df_tracking.loc[frames, 'visit_order'] = visit_order\n",
    "    df_tracking.loc[frames, 'visit_order_A'] = visit_order_A\n",
    "    df_tracking.loc[frames, 'visit_order_B'] = visit_order_B\n",
    "\n",
    "    # combine order to current and alternative\n",
    "    visits = df_tracking[df_tracking['transition_in']==1]\n",
    "    df_tracking['current_order'] = 0\n",
    "    df_tracking['alternative_order'] = 0\n",
    "    current_order = np.where(visits['location'] == 'platform A', visits['visit_order_A'], visits['visit_order_B'])\n",
    "    alternative_order = np.where(visits['location'] == 'platform A', visits['visit_order_B'], visits['visit_order_A'])\n",
    "    df_tracking.loc[frames, 'current_order'] = current_order\n",
    "    df_tracking.loc[frames, 'alternative_order'] = alternative_order\n",
    "\n",
    "    # add visit order without transitions \n",
    "    df_tracking['visit_order_noself'] = 0\n",
    "    visit_order_no_self = np.cumsum(visits['on_platform']-visits['transition_self'])\n",
    "    df_tracking.loc[frames, 'visit_order_noself'] = visit_order_no_self\n",
    "\n",
    "    # add visit status\n",
    "    df_tracking['status'] = 0\n",
    "    df_tracking.loc[frames, 'status'] = 1 # assume all visits are closed\n",
    "\n",
    "    # calculate visit length\n",
    "    df_tracking['visit_length'] = 0\n",
    "    visit_length = df_tracking[df_tracking['transition_out']==1].index - df_tracking[df_tracking['transition_in']==1].index\n",
    "    df_tracking.loc[frames, 'visit_length'] = [length/fps for length in visit_length]\n",
    "\n",
    "    # calculate visit latency\n",
    "    df_tracking['visit_latency'] = 0\n",
    "    visit_latency = np.diff(df_tracking[df_tracking['transition_in']==1].index, prepend= 0)\n",
    "    df_tracking.loc[frames, 'visit_latency'] = [latency/fps for latency in visit_latency]\n",
    "\n",
    "    # calculate travel time\n",
    "    df_tracking['travel_time'] = 0\n",
    "    if len(visit_length) > 1:\n",
    "        travel_time = list(visit_latency[1:] - visit_length[:-1])\n",
    "        travel_time.insert(0, visit_latency[0])\n",
    "    else:\n",
    "        travel_time = visit_latency\n",
    "    df_tracking.loc[frames, 'travel_time'] = [time/fps for time in travel_time]\n",
    "\n",
    "    return df_tracking, platform_A, platform_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add visualization \n",
    "df_tracking, platform_A, platform_B = track_visits(df_tracking, reference, fps, elevation_A, elevation_B)\n",
    "df_tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Head Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_head_direction(filtered_head, filtered_body, platform_A, platform_B):\n",
    "    # TODO\n",
    "    # calculate head direction\n",
    "    headcenter_med, headvectors, headdirection_mean = behaviorspecific.calculateheadvectors(filtered_head, head, ksmooth = 1)\n",
    "    \n",
    "    # TODO calculate body angle\n",
    "\n",
    "    # calculate direction of travel\n",
    "    bodycenter_med, kinematicvec = behaviorspecific.kinematicvector(filtered_body, n = 25)\n",
    "\n",
    "    # calculate projection angles towards palktforms\n",
    "    ref_platform_A = platform_A[0] + platform_A[1]/2 - platform_A[3]/2\n",
    "    ref_platform_B = platform_B[0] + platform_B[1]/2 - platform_B[3]/2\n",
    "    head_angles_A = behaviorspecific.projectionangle(headcenter_med, headvectors, ref_platform_A)\n",
    "    head_angles_B = behaviorspecific.projectionangle(headcenter_med, headvectors, ref_platform_B)\n",
    "    kinematic_angles_A = behaviorspecific.projectionangle(bodycenter_med, kinematicvec, ref_platform_A)\n",
    "    kinematic_angles_B = behaviorspecific.projectionangle(bodycenter_med, kinematicvec, ref_platform_B)\n",
    "\n",
    "    # Add parameters to df_tracking\n",
    "\n",
    "    return df_tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add visualization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Pecking Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_pecking(df_tracking, fps, platform_A, platform_B):\n",
    "    \n",
    "    # extract centroid_head\n",
    "    centroid_head = df_tracking[['centroid_head_x', 'centroid_head_y', 'centroid_head_z']]\n",
    "    on_platform_A = list(df_tracking['on_platform_A'])\n",
    "    on_platform_B = list(df_tracking['on_platform_B'])\n",
    "\n",
    "    # use platform points 0,1,3 because 2 is the height\n",
    "    distance_to_A = behaviorspecific.distance_to_plane(platform_A[0,:], platform_A[1,:], platform_A[3,:], centroid_head)\n",
    "    distance_to_B = behaviorspecific.distance_to_plane(platform_B[0,:], platform_B[1,:], platform_B[3,:], centroid_head)\n",
    "\n",
    "    # consider separating per platform here\n",
    "    pecktimes_A = behaviorspecific.find_pecks(distance_to_A, fps)\n",
    "    pecktimes_B = behaviorspecific.find_pecks(distance_to_B, fps)\n",
    "\n",
    "    # check if pecks in pecktimes are on platform\n",
    "    corrected_pecktimes_A = []\n",
    "    for peck in pecktimes_A:\n",
    "        if on_platform_A[peck]:\n",
    "            corrected_pecktimes_A.append(peck)\n",
    "\n",
    "    corrected_pecktimes_B = []\n",
    "    for peck in pecktimes_B:\n",
    "        if on_platform_B[peck]:\n",
    "            corrected_pecktimes_B.append(peck)\n",
    "\n",
    "    pecktimes = corrected_pecktimes_A + corrected_pecktimes_B\n",
    "\n",
    "    # extract peck positions\n",
    "    peckpositions = centroid_head.iloc[pecktimes]\n",
    "    peckpositions.columns=['peckposition_x', 'peckposition_y', 'peckposition_z']\n",
    "\n",
    "    # calculate distance to platform\n",
    "    distance_to_B[~np.array(on_platform_B)] = np.nan\n",
    "    distance_to_A[~np.array(on_platform_A)] = np.nan\n",
    "    distance_to_platform = np.where(np.isnan(distance_to_A), distance_to_B, distance_to_A)\n",
    "\n",
    "    # save tracking to data frame\n",
    "    df_tracking['distance_to_platform'] = distance_to_platform\n",
    "    df_tracking['peck'] = 0\n",
    "    df_tracking.loc[pecktimes,'peck'] = 1\n",
    "    df_tracking = pd.concat([df_tracking, peckpositions], axis=1, join='outer')\n",
    "\n",
    "    return df_tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracking = track_pecking(df_tracking, fps, platform_A, platform_B)\n",
    "df_tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_session_pervisits(df_tracking, fps):\n",
    "    '''\n",
    "    Return pandas DF with aggregate statistics for each session\n",
    "    '''\n",
    "\n",
    "    # specify percentage of visits to add as censor\n",
    "    p_censor = 0.5\n",
    "    IPI_bins = [.46, 1.1, 1.9]\n",
    "    IPD_bins = [55, 165, 260]\n",
    "\n",
    "    # get transition indices\n",
    "    idx_in = df_tracking[df_tracking['transition_in']==1].index\n",
    "    idx_out = df_tracking[df_tracking['transition_out']==1].index\n",
    "    idx_out_pre = np.insert(idx_out,0,0)\n",
    "\n",
    "    # calculate visit aggregates \n",
    "    visit_head_disp = []\n",
    "    visit_head_speed = []\n",
    "    visit_peck_count = []\n",
    "    visit_peck_rate = []\n",
    "    visit_IPI_entropy = []\n",
    "    visit_IPI_RMSSD = []\n",
    "    visit_IPD_entropy = []\n",
    "    visit_IPD_RMSSD = []\n",
    "\n",
    "    for a, b in zip(idx_in, idx_out):\n",
    "        visit = df_tracking.loc[a:b]\n",
    "        # head displacement\n",
    "        head_disp = np.sum(visit['euclidean_head_disp'])\n",
    "        visit_head_disp.append(head_disp)\n",
    "        # head speed\n",
    "        head_speed = np.mean(np.abs(np.diff(visit['euclidean_head_disp'])))*fps\n",
    "        visit_head_speed.append(head_speed)\n",
    "        # peck count\n",
    "        peck_count = np.sum(visit['peck'])\n",
    "        visit_peck_count.append(int(peck_count))\n",
    "        # normalized peck rate\n",
    "        peck_rate = peck_count/len(visit)*fps\n",
    "        visit_peck_rate.append(peck_rate)\n",
    "        if peck_count > 2:\n",
    "            # Inter peck interval\n",
    "            ipi = np.diff(visit[visit['peck']==1].index) / fps # in sec\n",
    "            # calculate RMSSD\n",
    "            visit_IPI_RMSSD.append(np.sqrt(np.mean(ipi**2)))\n",
    "            # quantize IPI\n",
    "            IPI_steps = np.digitize(ipi, IPI_bins)\n",
    "            value,counts = np.unique(IPI_steps, return_counts=True)\n",
    "            # calculate entropy\n",
    "            visit_IPI_entropy.append(entropy(counts, base=None))\n",
    "            # Inter peck distance\n",
    "            peckposition = visit[visit['peck']==1][visit.columns[visit.columns.str.contains('peckposition')]]\n",
    "            ipd = np.sqrt(np.sum(np.diff(peckposition, axis=0)**2, axis=1)) # in mm\n",
    "            # calculate RMSSD\n",
    "            visit_IPD_RMSSD.append(np.sqrt(np.mean(ipd**2)))\n",
    "            # quantize IPD\n",
    "            IPD_steps = np.digitize(ipd, IPD_bins)\n",
    "            values, counts = np.unique(IPD_steps, return_counts=True)\n",
    "            # calculate entropy\n",
    "            visit_IPD_entropy.append(entropy(counts, base=None))\n",
    "        else:\n",
    "            visit_IPI_entropy.append(0)\n",
    "            visit_IPI_RMSSD.append(0)\n",
    "            visit_IPD_entropy.append(0)\n",
    "            visit_IPD_RMSSD.append(0)\n",
    "    \n",
    "    # calculate travel aggregates\n",
    "    travel_body_disp = []\n",
    "    travel_body_speed = []\n",
    "    for c, d in zip(idx_out_pre, idx_in):\n",
    "        travel = df_tracking.loc[c:d]\n",
    "        # body displacement\n",
    "        body_disp = np.sum(travel['euclidean_body_disp'])\n",
    "        travel_body_disp.append(body_disp)\n",
    "        # calculate travel speed\n",
    "        body_speed = np.mean(abs(np.diff(travel['euclidean_body_disp'])))*fps\n",
    "        travel_body_speed.append(body_speed)\n",
    "        # TODO\n",
    "        # calcualte travel directedness (see kinematic vector)\n",
    "        # calculate haed direction \n",
    "    \n",
    "    # add parameters to df\n",
    "    df_tracking.loc[idx_in, 'head_disp'] = visit_head_disp\n",
    "    df_tracking.loc[idx_in, 'head_speed'] = visit_head_speed\n",
    "    df_tracking.loc[idx_in, 'peck_count'] = visit_peck_count\n",
    "    df_tracking.loc[idx_in, 'peck_rate'] = visit_peck_rate\n",
    "    df_tracking.loc[idx_in, 'IPI_entropy'] = visit_IPI_entropy\n",
    "    df_tracking.loc[idx_in, 'IPI_rmssd'] = visit_IPI_RMSSD\n",
    "    df_tracking.loc[idx_in, 'IPD_entropy'] = visit_IPD_entropy\n",
    "    df_tracking.loc[idx_in, 'IPD_rmssd'] = visit_IPD_RMSSD\n",
    "    df_tracking.loc[idx_in, 'travel_dist'] = travel_body_disp\n",
    "    df_tracking.loc[idx_in, 'travel_speed'] = travel_body_speed\n",
    "\n",
    "    # Filter visits\n",
    "    visits = df_tracking.loc[idx_in]\n",
    "\n",
    "    # Add cummulative parameters from last platform, alternative\n",
    "    cum_visit_length = []\n",
    "    cum_visits = []\n",
    "    cum_head_disp = []\n",
    "    cum_peck_count = []\n",
    "    cum_travel_dist = []\n",
    "    cum_travel_time = []\n",
    "    alt_cum_visit_length = []\n",
    "    alt_cum_visits = []\n",
    "    alt_cum_head_disp = []\n",
    "    alt_cum_peck_count = []\n",
    "    alt_cum_travel_dist = []\n",
    "    alt_cum_travel_time = []\n",
    "\n",
    "    for visit in visits.index:\n",
    "        previous_visits = visits.loc[:visit-1]\n",
    "        current_visits = visits.loc[:visit]\n",
    "        # check if current visit is platform A or platform B\n",
    "        if visits.loc[visit,'on_platform_A']:\n",
    "            # assign history of alternative patch\n",
    "            alt_prev_visits = previous_visits[previous_visits['on_platform_B']]\n",
    "            curr_prev_visits = current_visits[current_visits['on_platform_A']]\n",
    "        else:\n",
    "            alt_prev_visits = previous_visits[previous_visits['on_platform_A']]\n",
    "            curr_prev_visits = current_visits[current_visits['on_platform_B']]\n",
    "\n",
    "        if alt_prev_visits.empty:\n",
    "            cum_visit_length.append(np.max(np.cumsum(curr_prev_visits['visit_length'])))\n",
    "            cum_visits.append(len(curr_prev_visits))\n",
    "            cum_head_disp.append(np.max(np.cumsum(curr_prev_visits['head_disp'])))\n",
    "            cum_peck_count.append(np.max(np.cumsum(curr_prev_visits['peck_count'])))\n",
    "            cum_travel_dist.append(np.max(np.cumsum(curr_prev_visits['travel_dist'])))\n",
    "            cum_travel_time.append(np.max(np.cumsum(curr_prev_visits['travel_time'])))\n",
    "            # replace with 0 for empty history\n",
    "            alt_cum_visit_length.append(0)\n",
    "            alt_cum_visits.append(0)\n",
    "            alt_cum_head_disp.append(0)\n",
    "            alt_cum_peck_count.append(0)\n",
    "            alt_cum_travel_dist.append(0)\n",
    "            alt_cum_travel_time.append(0)\n",
    "        else:\n",
    "            cum_visit_length.append(np.max(np.cumsum(curr_prev_visits['visit_length'])))\n",
    "            cum_visits.append(len(curr_prev_visits))\n",
    "            cum_head_disp.append(np.max(np.cumsum(curr_prev_visits['head_disp'])))\n",
    "            cum_peck_count.append(np.max(np.cumsum(curr_prev_visits['peck_count'])))\n",
    "            cum_travel_dist.append(np.max(np.cumsum(curr_prev_visits['travel_dist'])))\n",
    "            cum_travel_time.append(np.max(np.cumsum(curr_prev_visits['travel_time'])))\n",
    "            alt_cum_visit_length.append(np.max(np.cumsum(alt_prev_visits['visit_length'])))\n",
    "            alt_cum_visits.append(len(alt_prev_visits))\n",
    "            alt_cum_head_disp.append(np.max(np.cumsum(alt_prev_visits['head_disp'])))\n",
    "            alt_cum_peck_count.append(np.max(np.cumsum(alt_prev_visits['peck_count'])))\n",
    "            alt_cum_travel_dist.append(np.max(np.cumsum(alt_prev_visits['travel_dist'])))\n",
    "            alt_cum_travel_time.append(np.max(np.cumsum(alt_prev_visits['travel_time'])))\n",
    "\n",
    "    # assign history on alternative patch\n",
    "    visits['cum_visit_length'] = cum_visit_length\n",
    "    visits['cum_visits'] = cum_visits\n",
    "    visits['cum_head_disp'] = cum_head_disp\n",
    "    visits['cum_peck_count'] = cum_peck_count\n",
    "    visits['cum_travel_dist'] = cum_travel_dist\n",
    "    visits['cum_travel_time'] = cum_travel_time\n",
    "    visits['alt_cum_visit_length'] = alt_cum_visit_length\n",
    "    visits['alt_cum_visits'] = alt_cum_visits\n",
    "    visits['alt_cum_head_disp'] = alt_cum_head_disp\n",
    "    visits['alt_cum_peck_count'] = alt_cum_peck_count\n",
    "    visits['alt_cum_travel_dist'] = alt_cum_travel_dist\n",
    "    visits['alt_cum_travel_time'] = alt_cum_travel_time\n",
    "    \n",
    "    # Add censoring for unfinished visits\n",
    "    # subset 50% of longest visits on each platform\n",
    "    visits_on_A = visits[visits['on_platform_A'] == 1]\n",
    "    visits_on_B = visits[visits['on_platform_B'] == 1]\n",
    "    filtered_visits_A = visits_on_A[visits_on_A['visit_length'] > visits_on_A['visit_length'].median()]\n",
    "    filtered_visits_B = visits_on_B[visits_on_B['visit_length'] > visits_on_B['visit_length'].median()]\n",
    "    combined_visits = pd.concat([filtered_visits_A, filtered_visits_B]).index\n",
    "    # subset random 25% of total visits (50% of 50%, filtered for longer half and balanced between platforms)\n",
    "    censored_percent = p_censor /0.5 # only considering top 50% of visits\n",
    "    idx = np.random.choice(combined_visits, int(round(len(combined_visits)*censored_percent)), replace=False)\n",
    "    censored_visits = visits.loc[idx]\n",
    "    censored_visits['status'] = 0\n",
    "    \n",
    "    # reduce visit length randomly between .25 and .75\n",
    "    rand_reduce = np.random.uniform(.25, .75, len(censored_visits))\n",
    "    censored_visits['visit_length'] = censored_visits['visit_length'] - censored_visits['visit_length'] * rand_reduce\n",
    "    # recalculate parameters for censored visits\n",
    "    idx_in_cens = censored_visits.index\n",
    "    idx_out_cens = round(idx_in_cens + censored_visits['visit_length']*fps).astype(int).values\n",
    "    censored_head_disp = []\n",
    "    censored_head_speed = []\n",
    "    censored_peck_count = []\n",
    "    censored_peck_rate = []\n",
    "    censored_IPI_entropy = []\n",
    "    censored_IPI_RMSSD = []\n",
    "    censored_IPD_entropy = []\n",
    "    censored_IPD_RMSSD = []\n",
    "\n",
    "    for a, b in zip(idx_in_cens, idx_out_cens):\n",
    "        visit = df_tracking.loc[a:b]\n",
    "        # head displacement\n",
    "        head_disp = np.sum(visit['euclidean_head_disp'])\n",
    "        censored_head_disp.append(head_disp)\n",
    "        # head speed\n",
    "        head_speed = np.mean(np.abs(np.diff(visit['euclidean_head_disp'])))*fps\n",
    "        censored_head_speed.append(head_speed)\n",
    "        # peck count\n",
    "        peck_count = np.sum(visit['peck'])\n",
    "        censored_peck_count.append(int(peck_count))\n",
    "        # normalized peck rate\n",
    "        peck_rate = peck_count/len(visit)*fps\n",
    "        censored_peck_rate.append(peck_rate)\n",
    "        if peck_count > 2:\n",
    "             # Inter peck interval\n",
    "            ipi = np.diff(visit[visit['peck']==1].index) / fps # in sec\n",
    "            # calculate RMSSD\n",
    "            censored_IPI_RMSSD.append(np.sqrt(np.mean(ipi**2)))\n",
    "            # quantize IPI\n",
    "            IPI_steps = np.digitize(ipi, IPI_bins)\n",
    "            value,counts = np.unique(IPI_steps, return_counts=True)\n",
    "            # calculate entropy\n",
    "            censored_IPI_entropy.append(entropy(counts, base=None))\n",
    "            # Inter peck distance\n",
    "            peckposition = visit[visit['peck']==1][visit.columns[visit.columns.str.contains('peckposition')]]\n",
    "            ipd = np.sqrt(np.sum(np.diff(peckposition, axis=0)**2, axis=1)) # in mm\n",
    "            # calculate RMSSD\n",
    "            censored_IPD_RMSSD.append(np.sqrt(np.mean(ipd**2)))\n",
    "            # quantize IPD\n",
    "            IPD_steps = np.digitize(ipd, IPD_bins)\n",
    "            values, counts = np.unique(IPD_steps, return_counts=True)\n",
    "            # calculate entropy\n",
    "            censored_IPD_entropy.append(entropy(counts, base=None))                                      \n",
    "        else: \n",
    "            censored_IPD_entropy.append(0)\n",
    "            censored_IPD_RMSSD.append(0)\n",
    "            censored_IPI_entropy.append(0)\n",
    "            censored_IPI_RMSSD.append(0)\n",
    "\n",
    "    \n",
    "    # recalculate cummulative features for censored visits\n",
    "    cum_visit_length = []\n",
    "    cum_visits = []\n",
    "    cum_head_disp = []\n",
    "    cum_peck_count = []\n",
    "    cum_travel_dist = []\n",
    "    cum_travel_time = []\n",
    "\n",
    "    for visit in censored_visits.index:\n",
    "        current_visits = censored_visits.loc[:visit]\n",
    "        # check if current visit is platform A or platform B\n",
    "        if censored_visits.loc[visit,'on_platform_A']:\n",
    "            # assign history of alternative patch\n",
    "            curr_prev_visits = current_visits[current_visits['on_platform_A']]\n",
    "        else:\n",
    "            curr_prev_visits = current_visits[current_visits['on_platform_B']]\n",
    "\n",
    "        cum_visit_length.append(np.max(np.cumsum(curr_prev_visits['visit_length'])))\n",
    "        cum_visits.append(len(curr_prev_visits))\n",
    "        cum_head_disp.append(np.max(np.cumsum(curr_prev_visits['head_disp'])))\n",
    "        cum_peck_count.append(np.max(np.cumsum(curr_prev_visits['peck_count'])))\n",
    "        cum_travel_dist.append(np.max(np.cumsum(curr_prev_visits['travel_dist'])))\n",
    "        cum_travel_time.append(np.max(np.cumsum(curr_prev_visits['travel_time'])))\n",
    "\n",
    "    # add to censored visits\n",
    "    censored_visits['head_disp'] = censored_head_disp\n",
    "    censored_visits['head_speed'] = censored_head_speed\n",
    "    censored_visits['peck_count'] = censored_peck_count\n",
    "    censored_visits['peck_rate'] = censored_peck_rate\n",
    "    censored_visits['IPI_entropy'] = censored_IPI_entropy\n",
    "    censored_visits['IPI_rmssd'] = censored_IPI_RMSSD\n",
    "    censored_visits['IPD_entropy'] = censored_IPD_entropy\n",
    "    censored_visits['IPD_rmssd'] = censored_IPD_RMSSD\n",
    "    censored_visits['cum_visit_length'] = cum_visit_length\n",
    "    censored_visits['cum_visits'] = cum_visits\n",
    "    censored_visits['cum_head_disp'] = cum_head_disp\n",
    "    censored_visits['cum_peck_count'] = cum_peck_count\n",
    "    censored_visits['cum_travel_dist'] = cum_travel_dist\n",
    "    censored_visits['cum_travel_time'] = cum_travel_time\n",
    "\n",
    "    # join visits\n",
    "    visits = pd.concat([visits, censored_visits]).sort_index()\n",
    "\n",
    "    return df_tracking, visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracking, visits = process_session_pervisits(df_tracking, fps)\n",
    "visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set project parameters\n",
    "projectpath = r\"K:\\ForagingPlatformsArena_local\\Triangulation\"\n",
    "outputpath = r\"K:\\ForagingPlatformsArena_local\\Analysis\"\n",
    "\n",
    "# read project structure\n",
    "metadata, pose_files = read_project_structure(projectpath)\n",
    "\n",
    "# set recording parameters (do not include paranthesis in bodypart string)\n",
    "reference_points = ['cA','cB','cC','cD','cE','cF']\n",
    "head = ['Head', 'UpperCere', 'LowerCere', 'BeakTip', 'LeftEye', 'RightEye', 'UpperNeck']\n",
    "body = ['UpperSpine=LN', 'LowerSpine', 'MiddleSpine', 'UpperHalfSpine',\n",
    "        'LowerHalfSpine', 'TailLeft', 'TailRight', 'TailCenter']\n",
    "\n",
    "# Smoothing parameters\n",
    "fps = 50 # in Hz\n",
    "# NOTE first three videos on 20221019 are recorded at 100Hz \n",
    "\n",
    "for file in pose_files:\n",
    "    print(f'Processing file {file}')\n",
    "    PID = os.path.splitext(os.path.basename(file))[0].split('_')[-1]\n",
    "    date = os.path.splitext(os.path.basename(file))[0].split('_')[2]\n",
    "    if date == '20221019':\n",
    "        fps = 100\n",
    "        print('corrected fps')\n",
    "    else:\n",
    "        fps = 50\n",
    "    # read metadata\n",
    "    condition, session, food_consumed, depleted_A, depleted_B, depleted_high, depleted_low, sex, age, colony, cohort, normal_weight, weight, elevation_A, elevation_B = read_metadata(metadata, PID, date)\n",
    "    print(f'Pigeon: {PID}, Date: {date}, Session: {session}, Condition: {condition}, Food consumed: {food_consumed}')\n",
    "    # track pigeon\n",
    "    df_tracking, pose, reference, filtered_head, filtered_body = track_pigeon(file, fps, reference_points, head, body)\n",
    "    # track platform visits\n",
    "    df_tracking, platform_A, platform_B = track_visits(df_tracking, reference, fps, elevation_A, elevation_B)\n",
    "    # track pecking behavior\n",
    "    df_tracking = track_pecking(df_tracking, fps, platform_A, platform_B)\n",
    "    # aggregate features\n",
    "    df_tracking, visits = process_session_pervisits(df_tracking, fps)\n",
    "    # add metadata to processed data\n",
    "    visits['PID'] = PID\n",
    "    visits['date'] = date\n",
    "    visits['session'] = session[0]\n",
    "    visits['condition'] = condition\n",
    "    visits['sex'] = sex[0]\n",
    "    visits['age'] = age[0]\n",
    "    visits['colony'] = colony[0]\n",
    "    visits['cohort'] = cohort[0]\n",
    "    visits['normal_weight'] = normal_weight[0]\n",
    "    visits['weight'] = weight[0]\n",
    "    visits['food_consumed'] = food_consumed\n",
    "    visits['depleted_A'] = depleted_A\n",
    "    visits['depleted_B'] = depleted_B\n",
    "    visits['depleted_high'] = depleted_high\n",
    "    visits['depleted_low'] = depleted_low\n",
    "    visits['elevation_A'] = elevation_A\n",
    "    visits['elevation_B'] = elevation_B\n",
    "    visits['total_latency'] = visits.index/fps\n",
    "    # change order of columns\n",
    "    visits = visits[['date', 'PID', 'session', 'condition', 'sex', 'age', 'colony', 'cohort', 'normal_weight', 'weight', \n",
    "                     'food_consumed', 'depleted_A', 'depleted_B', 'depleted_high', 'depleted_low', 'status',\n",
    "                     'visit_order', 'visit_order_noself', 'current_order', 'alternative_order', 'location', 'elevation', \n",
    "                     'transition_self', 'visit_length', 'visit_latency', 'total_latency', 'travel_time', 'head_disp', 'head_speed',\n",
    "                     'peck_count', 'peck_rate', 'IPI_entropy', 'IPI_rmssd', 'IPD_entropy', 'IPD_rmssd',\n",
    "                     'travel_dist', 'travel_speed', 'cum_visit_length', 'cum_visits', 'cum_head_disp', \n",
    "                     'cum_peck_count', 'cum_travel_dist', 'cum_travel_time', 'alt_cum_visit_length', 'alt_cum_visits', \n",
    "                     'alt_cum_head_disp', 'alt_cum_peck_count', 'alt_cum_travel_dist', 'alt_cum_travel_time']]\n",
    "       \n",
    "    # save processed data\n",
    "    filename = os.path.join(outputpath,os.path.splitext(os.path.basename(file))[0] + '_visits.csv')\n",
    "    visits.to_csv(filename, sep=',', index=False)\n",
    "    # save df tracking\n",
    "    filename = os.path.join(outputpath,os.path.splitext(os.path.basename(file))[0] + '_tracking.pkl')\n",
    "    df_tracking.to_pickle(filename)\n",
    "    \n",
    "    print(f'Data saved!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate all files into single dataset\n",
    "processed_files = utils.scrapdirbystring(outputpath, '_visits.csv', output = False)\n",
    "\n",
    "# read all csv files and merge into single dataset\n",
    "dataset = pd.DataFrame()\n",
    "for file in processed_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dataset = pd.concat([dataset, df], ignore_index=True)\n",
    "\n",
    "# save aggregate\n",
    "filename = os.path.join(outputpath,'AggregateVisits.csv')\n",
    "dataset.to_csv(filename, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis: \n",
    "More exploratory behavior towards the end, variability of behavior: When outside of a patch, the pigeons should move toward patch more often in the first third of a session and move away from patch more often in the last third of a session. Head movement and locomotion variability lower during time on task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tracking data\n",
    "projectpath = r\"K:\\ForagingPlatformsArena_local\\Triangulation\"\n",
    "outputpath = r\"K:\\ForagingPlatformsArena_local\\Analysis\"\n",
    "tracked_files = utils.scrapdirbystring(outputpath, 'tracking.pkl', output = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracking = pd.read_pickle(tracked_files[5])\n",
    "df_tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute spatiotemporal bins for IPI and IPD entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all data and merge ipi and ipd\n",
    "IPI = []\n",
    "IPD = []\n",
    "for file in tracked_files:\n",
    "    date = os.path.splitext(os.path.basename(file))[0].split('_')[2]\n",
    "    if date == '20221019':\n",
    "        fps = 100\n",
    "        print('corrected fps')\n",
    "    else:\n",
    "        fps = 50\n",
    "    df_tracking = pd.read_pickle(file)\n",
    "    idx_in = df_tracking[df_tracking['transition_in']==1].index\n",
    "    idx_out = df_tracking[df_tracking['transition_out']==1].index\n",
    "    for a, b in zip(idx_in, idx_out):\n",
    "        visit = df_tracking.loc[a:b]\n",
    "        peck_count = np.sum(visit['peck'])\n",
    "        if peck_count > 1:\n",
    "            # Inter peck interval\n",
    "            ipi = np.diff(visit[visit['peck']==1].index) / fps\n",
    "            # Inter peck distance\n",
    "            peckposition = visit[visit['peck']==1][visit.columns[visit.columns.str.contains('peckposition')]]\n",
    "            ipd = np.sqrt(np.sum(np.diff(peckposition, axis=0)**2, axis=1))\n",
    "            # save\n",
    "            IPI.append(ipi)\n",
    "            IPD.append(ipd)\n",
    "# flatten list of lists\n",
    "IPI = [item for sublist in IPI for item in sublist]\n",
    "IPD = [item for sublist in IPD for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IPD boundries based on hole distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.hist(IPD, bins = 200);\n",
    "plt.ylim(0, 800)\n",
    "plt.axvline(10, color='k', linestyle='--') # same hole\n",
    "plt.axvline(100, color='r', linestyle='--') # one hole cross\n",
    "plt.axvline(140, color='r', linestyle='--') # one hole diagonal\n",
    "plt.axvline(200, color='g', linestyle='--') # two holes cross\n",
    "plt.axvline(240, color='g', linestyle='--') # one holes cross one diagonal\n",
    "plt.axvline(280, color='g', linestyle='--') # two holes diagonal\n",
    "plt.axvline(300, color='b', linestyle='--') # more than two holes\n",
    "plt.axvline(780, color='b', linestyle='--') # max platform diagonal\n",
    "# minima boundries\n",
    "plt.axvline(55, color='k', linestyle='-')\n",
    "plt.axvline(165, color='k', linestyle='-')\n",
    "plt.axvline(260, color='k', linestyle='-')\n",
    "\n",
    "plt.xlabel('Inter-Peck Distance (mm)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPD_bins = [55, 165, 260]\n",
    "IPD_steps = np.digitize(IPD, IPD_bins)\n",
    "values, counts = np.unique(IPD_steps, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IPI boundries based on peck times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.hist([x for x in IPI if x < 5], bins = 500);\n",
    "plt.axvline(0.2, color='k', linestyle='-') # lower boundry\n",
    "plt.axvline(0.31, color='r', linestyle='--') # expected by juan delius 2003 (https://kops.uni-konstanz.de/server/api/core/bitstreams/8dae99c9-0774-412d-8086-61902f498130/content)\n",
    "plt.axvline(0.31*2, color='r', linestyle='--') # double expected\n",
    "plt.axvline(0.31*3, color='r', linestyle='--') # triple expected\n",
    "plt.axvline(0.31*4, color='r', linestyle='--') # quadruple expected\n",
    "plt.axvline(0.46, color='k', linestyle='-') # boundry more than 1x\n",
    "plt.axvline(1.1, color='k', linestyle='-') # boundry more than 3x\n",
    "plt.axvline(1.9, color='k', linestyle='-') # boundry more than 6x\n",
    "\n",
    "plt.xlabel('Inter-Peck Interval (sec)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPI_bins = [.46, 1.1, 1.9]\n",
    "IPI_steps = np.digitize(IPI, IPI_bins)\n",
    "values, counts = np.unique(IPI_steps, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPI_H = []\n",
    "IPD_H = []\n",
    "IPI_RMSSD = []\n",
    "IPD_RMSSD = []\n",
    "for file in tracked_files:\n",
    "    df_tracking = pd.read_pickle(file)\n",
    "    IPI_H.append(df_tracking['IPI_entropy'])\n",
    "    IPD_H.append(df_tracking['IPD_entropy'])\n",
    "    IPI_RMSSD.append(df_tracking['IPI_rmssd'])\n",
    "    IPD_RMSSD.append(df_tracking['IPD_rmssd'])\n",
    "# flatten list of lists\n",
    "IPI_H = [item for sublist in IPI_H for item in sublist]\n",
    "IPD_H = [item for sublist in IPD_H for item in sublist]\n",
    "IPI_RMSSD = [item for sublist in IPI_RMSSD for item in sublist]\n",
    "IPD_RMSSD = [item for sublist in IPD_RMSSD for item in sublist]\n",
    "\n",
    "# exclude nan values\n",
    "IPI_H = [x for x in IPI_H if not np.isnan(x)]\n",
    "IPD_H = [x for x in IPD_H if not np.isnan(x)]\n",
    "IPI_RMSSD = [x for x in IPI_RMSSD if not np.isnan(x)]\n",
    "IPD_RMSSD = [x for x in IPD_RMSSD if not np.isnan(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt scatterplot of IPI_H vs IPD_H\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "# add jitter to scatter\n",
    "x = [a + np.random.normal(0, 0.02) for a in IPI_H]\n",
    "y = [b + np.random.normal(0, 0.02) for b in IPD_H]\n",
    "plt.scatter(x, y, alpha=0.5, s = 5)\n",
    "plt.xlabel('IPI Entropy')\n",
    "plt.ylabel('IPD Entropy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt scatterplot of IPI_H vs IPD_H\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "# add jitter to scatter\n",
    "x = [a + np.random.normal(0, 0.02) for a in IPI_RMSSD]\n",
    "y = [b + np.random.normal(0, 0.02) for b in IPD_RMSSD]\n",
    "plt.scatter(x, y, alpha=0.5, s = 5)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel('IPI RMSSD')\n",
    "plt.ylabel('IPD RMSSD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time away from platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "# Create a function to resample a signal to a common length\n",
    "def resample_signal(signal, new_length):\n",
    "    old_indices = np.linspace(0, len(signal)-1, num=len(signal))\n",
    "    new_indices = np.linspace(0, len(signal)-1, num=new_length)\n",
    "    interpolator = interpolate.interp1d(old_indices, signal)\n",
    "    return interpolator(new_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeonplatform = pd.DataFrame()\n",
    "excluded = ['_P122_', '_P791_', '_P589_']\n",
    "for file in tracked_files:\n",
    "    date = os.path.splitext(os.path.basename(file))[0].split('_')[2]\n",
    "    if date == '20221019':\n",
    "        fps = 100\n",
    "        print('corrected fps')\n",
    "    else:\n",
    "        fps = 50\n",
    "    if any(x in file for x in excluded):\n",
    "        continue\n",
    "    # get metadata such as condition, session, food consumed, etc\n",
    "    PID = os.path.splitext(os.path.basename(file))[0].split('_')[-2]\n",
    "    condition, session, food_consumed, depleted_A, depleted_B, depleted_high, depleted_low, sex, age, colony, cohort, normal_weight, weight, elevation_A, elevation_B = read_metadata(metadata, PID, date)    # append to timeonplatform dataframe with metadata\n",
    "    if condition == '75-0':\n",
    "        condition = '0-75'\n",
    "    if food_consumed < 1:\n",
    "        continue\n",
    "    if condition == '0-75':\n",
    "        if depleted_A < 1:\n",
    "            continue\n",
    "        if depleted_B < 1:\n",
    "            continue\n",
    "    # read tracking data\n",
    "    df_tracking = pd.read_pickle(file)\n",
    "    # calculate cumsum of time on and off platform\n",
    "    cumsum_onplatform = np.cumsum(df_tracking['on_platform'])/fps\n",
    "    cumsum_offplatform = np.cumsum(~df_tracking['on_platform'])/fps\n",
    "    # resample to same length\n",
    "    cumsum_onplatform = resample_signal(cumsum_onplatform, 60000)\n",
    "    cumsum_offplatform = resample_signal(cumsum_offplatform, 60000)\n",
    "    # normalized cumsums\n",
    "    norm_cumsum_onplatform = cumsum_onplatform/(np.nanmax(cumsum_onplatform))\n",
    "    norm_cumsum_offplatform = cumsum_offplatform/(np.nanmax(cumsum_offplatform))\n",
    "    # calculate area under curve\n",
    "    AUC_onplatform = np.trapz(norm_cumsum_onplatform)\n",
    "    AUC_onplatform_firstthird = np.trapz(norm_cumsum_onplatform[0:len(cumsum_onplatform)//3])\n",
    "    AUC_onplatform_lastthird = np.trapz(norm_cumsum_onplatform[2*len(cumsum_onplatform)//3:len(cumsum_onplatform)])\n",
    "    AUC_offplatform = np.trapz(norm_cumsum_offplatform)\n",
    "    AUC_offplatform_firstthird = np.trapz(norm_cumsum_offplatform[0:len(cumsum_offplatform)//3])\n",
    "    AUC_offplatform_lastthird = np.trapz(norm_cumsum_offplatform[2*len(cumsum_offplatform)//3:len(cumsum_offplatform)])\n",
    "    ABC = AUC_onplatform - AUC_offplatform\n",
    "    ABC_firstthird = AUC_onplatform_firstthird - AUC_offplatform_firstthird\n",
    "    ABC_lastthird = AUC_onplatform_lastthird - AUC_offplatform_lastthird\n",
    "    # calculate area under curve from diagonal\n",
    "    diagonal = np.linspace(0, 1, num=60000)\n",
    "    AUC_reference = np.trapz(diagonal)\n",
    "    AUC_reference_firstthird = np.trapz(diagonal[0:len(diagonal)//3])\n",
    "    AUC_reference_lastthird = np.trapz(diagonal[2*len(diagonal)//3:len(diagonal)])\n",
    "    AUC_onplatform_ref = AUC_onplatform / AUC_reference\n",
    "    AUC_onplatform_firstthird_ref = AUC_onplatform_firstthird / AUC_reference_firstthird\n",
    "    AUC_onplatform_lastthird_ref = AUC_onplatform_lastthird / AUC_reference_lastthird\n",
    "    AUC_offplatform_ref = AUC_offplatform / AUC_reference\n",
    "    AUC_offplatform_firstthird_ref = AUC_offplatform_firstthird / AUC_reference_firstthird\n",
    "    AUC_offplatform_lastthird_ref = AUC_offplatform_lastthird / AUC_reference_lastthird\n",
    "\n",
    "    # save in loop\n",
    "    timeonplatform = timeonplatform.append({'PID': PID, 'date': date, 'condition': condition, 'session': int(session), 'performance': float(food_consumed/60), 'age': int(age), 'normal_weight': int(normal_weight), 'deprivation': float(weight/normal_weight),\n",
    "                                            'AUC_onplatform_ref': AUC_onplatform_ref, 'AUC_offplatform_ref': AUC_offplatform_ref, 'AUC_onplatform_firstthird_ref': AUC_onplatform_firstthird_ref, 'AUC_offplatform_firstthird_ref': AUC_offplatform_firstthird_ref,\n",
    "                                            'AUC_onplatform_lastthird_ref': AUC_onplatform_lastthird_ref, 'AUC_offplatform_lastthird_ref': AUC_offplatform_lastthird_ref, 'cumsum_onplatform': cumsum_onplatform, 'cumsum_offplatform': cumsum_offplatform,\n",
    "                                            'norm_cumsum_onplatform': norm_cumsum_onplatform, 'norm_cumsum_offplatform': norm_cumsum_offplatform}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data to aggregate on/off platform and first/last timepoint\n",
    "timeonplatform_long = pd.melt(timeonplatform, id_vars=['PID', 'date', 'condition', 'session', 'performance', 'age', 'normal_weight', 'deprivation'], \n",
    "                              value_vars=['AUC_onplatform_firstthird_ref', 'AUC_offplatform_firstthird_ref', 'AUC_onplatform_lastthird_ref', 'AUC_offplatform_lastthird_ref'], \n",
    "                              var_name='metric', value_name='AUC_norm')\n",
    "timeonplatform_long['timepoint'] = ['first 3rd' for x in timeonplatform_long['metric'] if 'firstthird' in x] + ['last 3rd' for x in timeonplatform_long['metric'] if 'last' in x]\n",
    "timeonplatform_long['allocation'] = ['on platform' if 'onplatform' in x else 'off platform' for x in timeonplatform_long['metric']]\n",
    "timeonplatform_long = timeonplatform_long.drop(columns=['metric'])\n",
    "timeonplatform_long\n",
    "# save as csv\n",
    "filename = os.path.join(outputpath,'Timeallocation.csv')\n",
    "timeonplatform_long.to_csv(filename, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute cumulative time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute \n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(21,7))\n",
    "for condition in set(timeonplatform['condition']):\n",
    "    # calculate mean and sem for each condition\n",
    "    subset = timeonplatform[timeonplatform['condition'] == condition]\n",
    "    unpacked_onplatform = list(zip(*subset['cumsum_onplatform']))\n",
    "    mean_onplatform = [np.mean(item) for item in unpacked_onplatform]\n",
    "    std_onplatform = [np.std(item) for item in unpacked_onplatform]\n",
    "    sem_onplatform = [std/np.sqrt(len(subset['cumsum_onplatform'])) for std in std_onplatform]\n",
    "    unpacked_offplatform = list(zip(*subset['cumsum_offplatform']))\n",
    "    mean_offplatform = [np.mean(item) for item in unpacked_offplatform]\n",
    "    std_offplatform = [np.std(item) for item in unpacked_offplatform]\n",
    "    sem_offplatform = [std/np.sqrt(len(subset['cumsum_offplatform'])) for std in std_offplatform]\n",
    "    mean_max = (np.max(mean_onplatform) + np.max(mean_offplatform))/2\n",
    "    if condition == '0-0':\n",
    "        # plot mean with ribbon in axis\n",
    "        ax1.plot(mean_onplatform, label='on platform', color = 'r')\n",
    "        ax1.fill_between(range(len(mean_onplatform)), np.array(mean_onplatform) - 1.96*np.array(sem_onplatform), np.array(mean_onplatform) + 1.96*np.array(sem_onplatform), alpha=0.3, color = 'r')\n",
    "        ax1.plot(mean_offplatform, label = 'off platform', color = 'b')\n",
    "        ax1.fill_between(range(len(mean_offplatform)), np.array(mean_offplatform) - 1.96*np.array(sem_offplatform), np.array(mean_offplatform) + 1.96*np.array(sem_offplatform), alpha=0.3, color = 'b')\n",
    "        # plot reference line from 0 to mean_max\n",
    "        ax1.plot([0,60000], [0,mean_max], 'k--')\n",
    "    elif condition == '0-75':\n",
    "        ax2.plot(mean_onplatform, label='on platform', color = 'r')\n",
    "        ax2.fill_between(range(len(mean_onplatform)), np.array(mean_onplatform) - 1.96*np.array(sem_onplatform), np.array(mean_onplatform) + 1.96*np.array(sem_onplatform), alpha=0.3, color = 'r')\n",
    "        ax2.plot(mean_offplatform, label = 'off platform', color = 'b')\n",
    "        ax2.fill_between(range(len(mean_offplatform)), np.array(mean_offplatform) - 1.96*np.array(sem_offplatform), np.array(mean_offplatform) + 1.96*np.array(sem_offplatform), alpha=0.3, color = 'b')\n",
    "        # plot reference line from 0 to mean_max\n",
    "        ax2.plot([0,60000], [0,mean_max], 'k--')\n",
    "    elif condition == '75-75':\n",
    "        ax3.plot(mean_onplatform, label='on platform', color = 'r')\n",
    "        ax3.fill_between(range(len(mean_onplatform)), np.array(mean_onplatform) - 1.96*np.array(sem_onplatform), np.array(mean_onplatform) + 1.96*np.array(sem_onplatform), alpha=0.3, color = 'r')\n",
    "        ax3.plot(mean_offplatform, label = 'off platform', color = 'b')\n",
    "        ax3.fill_between(range(len(mean_offplatform)), np.array(mean_offplatform) - 1.96*np.array(sem_offplatform), np.array(mean_offplatform) + 1.96*np.array(sem_offplatform), alpha=0.3, color = 'b')\n",
    "        # plot reference line from 0 to mean_max\n",
    "        ax3.plot([0,60000], [0,mean_max], 'k--')\n",
    "    ax1.set(title = '0-0', ylabel='cumulative time in seconds', xlabel = 'session time in frames', ylim = (0, 850), xlim = (0, 60000))\n",
    "    ax2.set(title = '0-75', ylabel='cumulative time in seconds', xlabel = 'session time in frames', ylim = (0, 850), xlim = (0, 60000))\n",
    "    ax3.set(title = '75-75', ylabel='cumulative time in seconds', xlabel = 'session time in frames', ylim = (0, 850), xlim = (0, 60000))\n",
    "    ax3.legend()\n",
    "    # set xticks in scientific notation\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "        ax.axvspan(20000, 40000, color='gray', alpha=0.1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data for boxplots: AUC_onplatform_firstthird_ref, AUC_offplatform_firstthird_ref\n",
    "# boxplot of timeonplatform['ABC'] by condition and by time on or off\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(21,7))\n",
    "\n",
    "colors = ['red', 'blue', 'red', 'blue']  # specify the colors you want\n",
    "alpha = 0.5\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Create a Patch object for each boxplot\n",
    "patches = [mpatches.Patch(color=color, label=label, alpha = 0.5) for color, label in zip(colors, ['on platform', 'off platform'])]\n",
    "\n",
    "# condition 0-0\n",
    "def colored_boxplot(ax, data, position, color, alpha=0.5):\n",
    "    box = ax.boxplot(data, patch_artist=True, notch=True, positions=[position], widths=0.5)\n",
    "    for patch in box['boxes']:\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(alpha)\n",
    "        patch.set_edgecolor('k')\n",
    "        patch.set_linewidth(2)\n",
    "    return box\n",
    "\n",
    "# condition 0-0\n",
    "ax1.axhline(1, color='k', linestyle='--')\n",
    "colored_boxplot(ax1, timeonplatform[timeonplatform['condition']=='0-0']['AUC_onplatform_firstthird_ref'], 1, colors[0], alpha)\n",
    "colored_boxplot(ax1, timeonplatform[timeonplatform['condition']=='0-0']['AUC_offplatform_firstthird_ref'], 2, colors[1], alpha)\n",
    "colored_boxplot(ax1, timeonplatform[timeonplatform['condition']=='0-0']['AUC_onplatform_lastthird_ref'], 3, colors[2], alpha)\n",
    "colored_boxplot(ax1, timeonplatform[timeonplatform['condition']=='0-0']['AUC_offplatform_lastthird_ref'], 4, colors[3], alpha)\n",
    "ax1.set(title = '0-0', ylim = (0, 3.5), ylabel = 'AUC relative to diagonal', xlabel = 'session time')\n",
    "ax1.set_xticklabels(['first third','', 'last third',''], ha = 'left')\n",
    "\n",
    "# condition 0-75\n",
    "ax2.axhline(1, color='k', linestyle='--')\n",
    "colored_boxplot(ax2, timeonplatform[timeonplatform['condition']=='0-75']['AUC_onplatform_firstthird_ref'], 1, colors[0], alpha)\n",
    "colored_boxplot(ax2, timeonplatform[timeonplatform['condition']=='0-75']['AUC_offplatform_firstthird_ref'], 2, colors[1], alpha)\n",
    "colored_boxplot(ax2, timeonplatform[timeonplatform['condition']=='0-75']['AUC_onplatform_lastthird_ref'], 3, colors[2], alpha)\n",
    "colored_boxplot(ax2, timeonplatform[timeonplatform['condition']=='0-75']['AUC_offplatform_lastthird_ref'], 4, colors[3], alpha)\n",
    "ax2.set(title = '0-75', ylim = (0, 3.5), ylabel = 'AUC relative to diagonal', xlabel = 'session time')\n",
    "ax2.set_xticklabels(['first third','', 'last third',''], ha = 'left')\n",
    "\n",
    "# condition 75-75\n",
    "ax3.axhline(1, color='k', linestyle='--')\n",
    "colored_boxplot(ax3, timeonplatform[timeonplatform['condition']=='75-75']['AUC_onplatform_firstthird_ref'], 1, colors[0], alpha)\n",
    "colored_boxplot(ax3, timeonplatform[timeonplatform['condition']=='75-75']['AUC_offplatform_firstthird_ref'], 2, colors[1], alpha)\n",
    "colored_boxplot(ax3, timeonplatform[timeonplatform['condition']=='75-75']['AUC_onplatform_lastthird_ref'], 3, colors[2], alpha)\n",
    "colored_boxplot(ax3, timeonplatform[timeonplatform['condition']=='75-75']['AUC_offplatform_lastthird_ref'], 4, colors[3], alpha)\n",
    "ax3.set(title = '75-75', ylim = (0, 3.5), ylabel = 'AUC relative to diagonal', xlabel = 'session time')\n",
    "ax3.set_xticklabels(['first third','', 'last third',''], ha = 'left')\n",
    "ax3.legend(handles=patches) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anipose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
